<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Academic Project Page</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Bohan Jia</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Wenxuan Huang</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yuntian Tang</a><sup>*</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name<br>Conferance name and year</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/BhJia/ComBench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While real-world applications increasingly demand intricate scene manipulation, existing instruction-guided image editing benchmarks often oversimplify task complexity and instruction comprehensiveness. To address this gap, we introduce Comp-Edit, a large-scale benchmark specifically designed for complex instruction-guided image editing. Our Comp-Edit contains complicated tasks requiring fine-grained instruction-following, spatial-contextual reasoning and precise editing capabilities of image editing model. Meanwhile, To better align instructions with complex editing requirements, we propose an instruction decoupling method that disentangles editing intents into four dimensions: location (spatial constraints), appearance (visual attributes), dynamics (temporal interactions), and objects (entity relationships). Through extensive evaluations, we demonstrate that Comp-Edit exposes fundamental limitations in current methods, which offers a critical tool for advancing next-generation image editing models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<section class="hero teaser">
  <!-- <section class="section"> -->
    <div class="container is-max-desktop">
      <!-- <div class="columns is-centered has-text-centered">-->
      <!-- <div class="column is-four-fifths"> -->
          <img src="static/images/teaser.png">
          <h3 class="subtitle has-text-centered">
            <small>
            <strong>MLLM-CompBench</strong> offers diverse triplets comprising two images, a question about their relativity, <br> and an answer to cover eight types of relativity.
            </small>
          </h3>
        <!-- </div> -->
      <!-- </div> -->
    <!-- </div> -->
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Dataset Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2> <br>

        <div class="content has-text-justified">
        <p>
          <strong>MLLM-CompBench</strong> comprises 39.8K triplets, each containing 1) a pair of visually or semantically relevant images, 2) a question about their relativity, and 3) a ground-truth answer.
          We consider a wide range of questions categorized into eight aspects of relativity. 
        </p>

        <div class="columns is-centered">
          <div class="column is-half has-text-centered">
            <img src="static/images/tasks.pdf" alt="Tasks Image">
          </div>
          <div class="column is-half has-text-centered">
            <img src="static/images/model_ssim_comparison_bubble.png" alt="Another Image">
          </div>
        </div>

        <p>
          <strong>Attribute Relativity</strong> tests the ability to recognize relative attributes such as size, color, texture, shape, and pattern. 
          For instance, given two images of birds, we ask MLLMs to compare the length of their beaks (e.g., "Which bird has longer beaks?").
          <strong>Existential Relativity</strong> assesses the comprehension of existence in comparisons, asking questions like "Which trait is in the left butterfly but not in the right butterfly?".
          <strong>State/Emotion Relativity</strong> examines if MLLMs can identify state variations, such as different degrees of baking and smiling.
          <strong>Temporal Relativity</strong> evaluates the understanding of time-related changes between two objects or scenes (e.g., "Which video frame happens earlier during a free kick?"). 
          <strong>Spatial Relativity</strong> checks the ability to tell spatial differences (e.g., "Which cup looks further?"). 
          Finally, <strong>Quantitiy/Quality Relativity</strong> investigates whether an MLLM understands the relativity of quantity and quality (e.g., "Which image contains more animal instances?").

        </p>
      </div>
    </div>
    <!--/ Dataset Overview. -->
  </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
      <!-- Curation Pipeline. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Data Curation</h2> <br>
  
          <div class="content has-text-justified">
            <p>
              The data curation pipeline for MLLM-CompBench includes data selection, question generation, answer annotation, and verification. 
              We rely on combinations of humans, computer programs, MLLMs (specifically GPT-4V), and CLIP similarity to select images and generate questions, based on relativity types and available metadata.
            </p>
  
          </div>
          <img src="static/images/Overall.pdf">
          <!--
          <h3 class="subtitle has-text-centered">
            <br>
            <small>CompBench Curation Pipeline.</small>
          </h3>-->
          
  
          
        </div>
      </div>
      <!--/ Curation Pipeline. -->
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
      <!-- Results. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experimental Results</h2> <br>
  

          <div class="content has-text-justified">
            <p>
              We evaluate four leading MLLMs (i.e., GPT-4V(ision),  Gemini1.0-Pro, LLaVA-1.6,  VILA-1.5) across <strong>eight</strong> relative comparisons spanning <strong>sixteen</strong> tasks.
              The top-performing model in each task is indicated <strong>in bold</strong>. 
              ST: MIT-States,
              FA: Fashionpedia,
              VA: VAW,
              CU: CUB-200-2011,
              WF: Wildfish++,
              MB: MagicBrush,
              SD: Spot-the-diff,
              CE: CelebA,
              FE: FER-2013,
              SN: SoccerNet,
              CC: CompCars,
              ND: NYU-Depth V2,
              VQ: VQAv2,
              QB: Q-Bench2.
            </p>
          </div>
            
          <img src="static/images/results.jpg">
          

          <div class="content has-text-justified">
            <p>
              We observe that current MLLMs face challenges in answering relative questions in MLLM-CompBench. 
              All MLLMs achieve averaged accuracies over the sixteen tasks (columns) below 80%, with GPT-4V reaching the highest accuracy at 74.7%.
              In particular, the existing MLLMs struggle with answering relative questions regarding Existence, Spatiality, and Quantity.
            </p>
          </div>
  
          <div class="content has-text-justified">
            <p>
              <strong>Error Analysis on MLLM-CompBench</strong>. We observe four types of errors where GPT4-V falls short: 
              (i) differentiating colors between objects and backgrounds, 
              (ii) counting small or distant objects, 
              (iii) identifying objects within crowded scenes, and 
              (iv) recognizing out-of-focus details.
            </p>
          </div>
          
          <img src="static/images/error_analysis.jpg">
          

          
          
        </div>
      </div>
      
      <!--/ Results. -->
    </div>

</section>




<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
